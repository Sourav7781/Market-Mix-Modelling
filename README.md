# Market-Mix-Modelling
Developing multivariate ML algorithms using Python to understand the ﻿ impact of advertising spend on sales and optimizing the advertising budget.


Marketing mix modeling (MMM) is a statistical analysis such as multivariate regressions on sales and marketing time series data to estimate the impact of various marketing tactics (marketing mix) on sales and then forecast the impact of future sets of tactics. It seeks to determine how much success was generated by each factor, and forecast what future success can be created through altering and optimising the marketing mix.

The key purpose: By estimating the effectiveness of different marketing channels activities, MMM helps to better understand how various marketing activities are driving the business metrics of a product and increase ROI (Return on investment).
MMM is similar to the concept of 4Ps of the marketing mix: Product, Price, Place, Promotion. It’s a fundamental part of marketing theory that considers what factors are required for a business to succeed.

Data Preparation and Data Transformation:
The industry standard typically will pick a weekly time period. This is because monthly data granularity is too long and daily level data has too much variation which leads to poor accuracy. Therefore, aggregate data at a weekly level is the best practice for creating a MMM model.

Benefits of Marketing Mix Modelling
Enables marketers to prove the ROI (Return of Investment) of their efforts
Returns insights that allow for effective budget allocation
Facilitates superior sales trend forecasting
Setup
First, we’re going to import the libraries and read the data, as usual.

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
import statsmodels.formula.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import PolynomialFeatures
from statsmodels.regression import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

from google.colab import drive
drive.mount('/content/drive')

df1 = pd.read_excel("/content/drive/MyDrive/DataBrio/MMM.xlsx",parse_dates=['Date'],index_col='Date')
df1.head()

df1.shape

df1.Influencer.value_counts()

There are 1154 Mega, 1149 Micro, 1136 Nano and 1113 Macro influencers present in this dataset

# Checking for Null values

df1.isnull().sum()

df1.describe()


# Checking Correlation

correlation=df1.corr()

sns.set(font_scale=1.2)
plt.figure(figsize=(10, 6))
sns.heatmap(correlation,cmap='bone',annot=True)

From the above plot we can say there is a higher positive correlation between TV and Sales (0.96) than between Radio and Sales (0.85) or Social_Media and Sales (0.54)

sns.pairplot(df1, hue="Influencer", size=1.5)

X = df1.drop(columns=['Sales','Influencer'])
y = df1['Sales']

lr = LinearRegression()

lr.fit(X, y)
print('Coefficients:', lr.coef_)
print('Intercept:', lr.intercept_)

print(cross_val_score(lr, X, y, cv=TimeSeriesSplit()))

cv_results = cross_val_score(lr, X, y, cv=TimeSeriesSplit())
np.mean(cv_results)

we can say this is not too bad model performance

model = sm.ols(formula="Sales ~ TV + Radio + Social_Media", data = df1).fit()
print(model.summary())

Sales = 85.1463 + 3.4555 * TV + 0.98 * Radio + 2.469 * Social Media

data1 = pd.concat([X, y], axis=1)

I just created here with only X $ Y variable by conacatening

X5 = X.copy()
vif = [variance_inflation_factor(X5.values, i) for i in range(X5.shape[1])]
pd.Series(vif,index=X5.columns)

X3 = X.copy()
y3 = y.copy()
y3 = list(y3)
x_interaction = PolynomialFeatures(2, interaction_only=True, include_bias=False).fit_transform(X3)
print(x_interaction)

X3 = X.copy()
y3 = y.copy()
y3 = list(y3)
x_interaction = PolynomialFeatures(2, interaction_only=True, include_bias=False).fit_transform(X3)
interaction_df = pd.DataFrame(x_interaction, columns = ['TV','Radio','Social_Media','TV:Radio','TV:Social_Media','Radio:Social_Media'])
interaction_model = linear_model.OLS(y3, interaction_df).fit()
interaction_model.pvalues[interaction_model.pvalues < 0.05]

print(interaction_model.summary())

X8 = interaction_df.copy()
vif = [variance_inflation_factor(X8.values, i) for i in range(X8.shape[1])]
pd.Series(vif,index=X8.columns)

interaction effect is present, we should invest in interaction terms
But there is more multicollinearity

weights = pd.Series(lr.coef_,index=X.columns)
base = lr.intercept_
unadj_contributions = X.mul(weights).assign(Base=base)
adj_contributions = (unadj_contributions.div(unadj_contributions.sum(axis=1), axis=0).mul(y, axis=0))
ax = (adj_contributions[['Base', 'Social_Media', 'Radio', 'TV']].plot.area(figsize=(10, 6),linewidth=0.5,title='Predicted Sales and Breakdown',ylabel='Sales',xlabel='Date'))
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles[::-1], labels[::-1],title='Channels', loc="center left",bbox_to_anchor=(1.01, 0.5))


here we can see my base line is around 80 to 90, SM is around 100 to 120, Radio is around 120 to 150, and TV is around 150 to 450


here in adjusted contribution base is changing because of correction factor,
Correction factor=model prediction/true target

print(weights)

print(base)

print(unadj_contributions)

unadj_contributions.sum(axis=1)

print(adj_contributions)

see here it is changing

adj_contributions['TV'].sum()

df1['TV'].sum()

sales_from_tv = adj_contributions['TV'].sum()
spendings_on_tv = df1['TV'].sum()
tv_roi = sales_from_tv / spendings_on_tv
sales_from_radio = adj_contributions['Radio'].sum()
spendings_on_radio = df1['Radio'].sum()
radio_roi = sales_from_radio / spendings_on_radio
sales_from_Social_Media = adj_contributions['Social_Media'].sum()
spendings_on_Social_Media = df1['Social_Media'].sum()
Social_Media_roi = sales_from_Social_Media / spendings_on_Social_Media

print('TV: ',tv_roi)
print('Radio: ',radio_roi)
print('SM: ',Social_Media_roi)

less than 1 means poor ROI

If we invest 1 $ on TV, we got 3 dollars 46 cents back.

infl = pd.get_dummies(df1.Influencer)
model_data = pd.concat([df1,infl], axis=1)
model_data.drop('Influencer',inplace=True,axis=1)
model_data.head()

X1 = model_data.loc[:, model_data.columns != 'Sales']
X1 = X1.loc[:, X1.columns != 'Mega']
y1 = model_data['Sales']

X_train, X_test, y_train, y_test = train_test_split(X1, y1 ,test_size=0.35, random_state=101)

lr2 = LinearRegression()
lr2.fit(X_train, y_train)
y_pred = lr2.predict(X_train)

from sklearn.metrics import r2_score
r_squared = r2_score(y_train, y_pred)
n = len(y_train)
k = X_train.shape[1]

adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)

print("Adjusted R-squared:", adjusted_r_squared)

mse = mean_squared_error(y_train, y_pred)
print("Mean Squared Error (MSE):", mse)

print(lr2.intercept_)

coeff_df = pd.DataFrame(lr2.coef_,X1.columns,columns=['Coefficient'])
coeff_df

We understand that the spends on TV have a positive causal relationship with sales, as an increase of 1 dollar in TV spends has an effect of 3.45 dollars increase in Sales. It is however interesting to note that Radio and Social media also have a positive impact on Sales as they increase the sales by 0.97 dollars and 2.61 dollars with every additional 1 Mn dollars spent. Now since Macro, Micro and Nano are binary variables, what their coefficients mean is that whenever the variable is flagged 1, i.e. suppose a Macro influencer was used, sales increased by around 2.63 dollars.

y_pred = lr2.predict(X_test)
print(y_pred)

from sklearn import metrics
print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
print('MSE:', metrics.mean_squared_error(y_test, y_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

1. Mean absolute error (MAE) is calculated as the mean of the absolute difference in the predicted sales of the test dataset and the actual values of Sales given. Here, there is a fluctuation of about 25.915 Mn dollars between the predicted and actual values of Sales.
2. Root Mean Square error (RMSE) is another transformed variation of measuring the fluctuations between actual and predicted sales. It is found to be $31.17 Mn, which is also close to the MAE reported.


we can say the linear model has performed average.


from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error as mae
from sklearn.model_selection import train_test_split
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=.25, random_state=0)

model2 = RandomForestRegressor(random_state=1)
model2.fit(X_train2, y_train2)
pred = model2.predict(X_test2)
feat_importances = pd.Series(model2.feature_importances_, index=X.columns)
feat_importances.nlargest(25).plot(kind='barh',figsize=(10,8))

feat_importances

There seems to be a pattern, where TV is the most important, followed by Social_Media, leaving radio last.

import time
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lars
from sklearn.linear_model import TheilSenRegressor
from sklearn.linear_model import HuberRegressor
from sklearn.linear_model import PassiveAggressiveRegressor
from sklearn.linear_model import ARDRegression
from sklearn.linear_model import BayesianRidge
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import OrthogonalMatchingPursuit
from sklearn.svm import SVR
from sklearn.svm import NuSVR
from sklearn.svm import LinearSVR
from sklearn.kernel_ridge import KernelRidge
from sklearn.isotonic import IsotonicRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_decomposition import PLSRegression

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

sns.distplot(X_test)

regressors = {
    "XGBRegressor": XGBRegressor(booster='gblinear', objective ='reg:squarederror', verbosity = 0, random_state=42),
    # reg:linear is now deprecated in favor of reg:squarederror
    # booster='gblinear' because by default it is set to 'gbtree'. 'gbtree' won't give us coefficients
    # verbosity = 0 -> Verbosity of printing messages. Valid values are 0 (silent/ignore errors), 1 (warning), 2 (info), 3 (debug).
    "RandomForestRegressor": RandomForestRegressor(),
    "DecisionTreeRegressor": DecisionTreeRegressor(),
    "GaussianProcessRegressor": GaussianProcessRegressor(),
    "SVR": SVR(),
    "NuSVR": NuSVR(),
    "LinearSVR": LinearSVR(),
    "KernelRidge": KernelRidge(),
    "LinearRegression": LinearRegression(),
    "Ridge": Ridge(),
    "Lars": Lars(),
    "TheilSenRegressor": TheilSenRegressor(),
    "HuberRegressor": HuberRegressor(),
    "PassiveAggressiveRegressor": PassiveAggressiveRegressor(),
    "ARDRegression": ARDRegression(),
    "BayesianRidge": BayesianRidge(),
    "ElasticNet": ElasticNet(),
    "OrthogonalMatchingPursuit": OrthogonalMatchingPursuit(),
    "pls": PLSRegression(),
}

We’ll create a Pandas dataframe called df_models and now loop through the regressors, fit each one to the data, and return the Root Mean Squared Error of a 10-fold cross validation to the dataframe, and then display the output.

import warnings
warnings.filterwarnings("ignore")  # doesn't spam the notebook with warnings

df_models = pd.DataFrame(columns=['model', 'run_time', 'rmse', 'rmse_cv'])
for key in regressors:
    #print('*',key)
    start_time = time.time()
    regressor = regressors[key]
    model3 = regressor.fit(X_train, y_train)
    y_pred = model3.predict(X_test)

    scores = cross_val_score(model3, X_train, y_train, scoring="neg_mean_squared_error", cv=10)

    row = {'model': key,'run_time': format(round((time.time() - start_time)/60,7)),
           'rmse': round(np.sqrt(mean_squared_error(y_test, y_pred))),'rmse_cv': round(np.mean(np.sqrt(-scores)))}

    df_models = df_models.append(row, ignore_index=True)

print(scores)

print(-scores)

df_models.head(20).sort_values(by='rmse_cv', ascending=True)

from sklearn.linear_model import RidgeCV

# Choosing best alpha for ridge regression with Leave one out cross-validation
ridge_cv = RidgeCV(alphas=(0.01, 0.1, 1, 10, 100), scoring='neg_mean_squared_error', store_cv_values=True).fit(X_train, y_train)

# Best alpha
ridge_cv.alpha_

regressor = Ridge(alpha=0.1)
model4 = regressor.fit(X_train, y_train)
y_pred = model4.predict(X_test)
y_pred

print(regressor)

# Defining Actual and Predicted values

test2 = pd.DataFrame({'Predicted sales':y_pred, ' Actual sales':y_test})
fig= plt.figure(figsize=(12,6))
test2 = test2.reset_index()
test2 = test2.drop(['Date'],axis=1)
plt.plot(test2[:50])
plt.legend(['Actual sales','Predicted sales'])

test2.head()

class ExponentialSaturation:
    def __init__(self, a=1.):
        self.a = a

    def transform(self, X):
        return 1 - np.exp(-self.a*X)

However, we will add some sanity checks for the inputs to make it scikit-learn compliant. This bloats up the code a bit, but it is a comparatively small price that we have to pay.

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted, check_array
class ExponentialSaturation(BaseEstimator, TransformerMixin):
    def __init__(self, a=1.):
        self.a = a

    def fit(self, X, y=None):
        X = check_array(X)
        self._check_n_features(X, reset=True) # from BaseEstimator
        return self
    def transform(self, X):
        check_is_fitted(self)
        X = check_array(X)
        self._check_n_features(X, reset=False) # from BaseEstimator
        return 1 - np.exp(-self.a*X)

It is still not perfect because we should also implement a check for a being larger than zero, but this is something we can easily do on our own. With the ExponentialSaturation transformer, we can do the following:

![image](https://miro.medium.com/max/452/1*hgHVvbOuRoQyMxW2mUNZww.png)

Let us now move on to the next effect.

# Creating a Carry-Over Effect
This one is slightly more tricky. Let us use an example to understand what we want to achieve. We are given a series of spendings over time such as
### (16, 0, 0, 0, 0, 4, 8, 0, 0, 0),
meaning that we spent 16 in the first week, then we spent nothing from week 2 to 5, then we spent 4 in week 6, etc.

We now want that spendings in one week to get partially carried over to the next weeks in an exponential fashion. This means: In week 1 there is a spend of 16. Then we carry over 50%, meaning
- 0.5 * 16 = 8 to week 2,
- 0.5² * 16 = 4 to week 3,
- 0.5³ * 16 = 2 to week 4,
- …

## This introduces two hyperparameters:
<b>The strength (how much gets carried over?)</b> and <b>the length</b> (how long does it get carried over?) of the carry-over effect. If we use a strength of 50% and a length of 2, the spending sequence from above becomes
### (16, 8, 4, 0, 0, 4, 10, 5, 2, 0).

We can write some loops to implement this behavior, a nice and fast way is using convolutions though. We will not explain it in detail.

from scipy.signal import convolve2d
import numpy as np
class ExponentialCarryover(BaseEstimator, TransformerMixin):
    def __init__(self, strength=0.5, length=1):
        self.strength = strength
        self.length = length
    def fit(self, X, y=None):
        X = check_array(X)
        self._check_n_features(X, reset=True)
        self.sliding_window_ = (
            self.strength ** np.arange(self.length + 1)
        ).reshape(-1, 1)
        return self
    def transform(self, X: np.ndarray):
        check_is_fitted(self)
        X = check_array(X)
        self._check_n_features(X, reset=False)
        convolution = convolve2d(X, self.sliding_window_)
        if self.length > 0:
            convolution = convolution[: -self.length]
        return convolution

We can see that the class takes the strength and length. During the fit it creates a sliding window that gets used by the convolve2d function, doing magically just what we want. If we know convolutional layers from CNNs, this is exactly what happens here. Graphically, it does the following:

![image](https://miro.medium.com/max/656/1*8AEZKG-bjymGEHHa-k-YuQ.gif)

> <b>Note</b>: There are many more ways to create a carryover as well. The decay does not have to be exponential. And maybe the peak of the advertising effect is not reached on the day the money was spent, but always the next week. We can express all of these variations by changing the sliding window accordingly.
Let us combine both saturation and carry-over effects to create a more realistic marketing mix model.

# The Final Model
We will use different saturations and carryovers for each channel. This makes sense because usually, a TV ad sticks longer in our head than a banner that we see online, for example. From a high-level perspective, the model will look like this:
![image](https://miro.medium.com/max/700/1*ZitGnL3nODgKcdM7xKPqug.png)

Note that the blue pipeline is just a function of the TV spendings, the orange one of radio spendings, and the purple one of banner spendings. We can efficiently implement this in scikit-learn using the <b>ColumnTransformer</b> and <b>Pipeline</b> classes. The <b>ColumnTransformer</b> allows us to use a different transformation for each ad channel while the <b>Pipeline</b> allows us to chain operations for a single channel. Take a second to understand the following snippet:

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Applying adstock effects to each channel via Pipeline with ColumnTransformer
adstock =ColumnTransformer(
    [
     ('tv_pipe', Pipeline([
                           ('carryover', ExponentialCarryover()),
                           ('saturation', ExponentialSaturation())
     ]), ['TV']),
     ('radio_pipe', Pipeline([
                           ('carryover', ExponentialCarryover()),
                           ('saturation', ExponentialSaturation())
     ]), ['Radio']),
     ('Social_Media_pipe', Pipeline([
                           ('carryover', ExponentialCarryover()),
                           ('saturation', ExponentialSaturation())
     ]), ['Social_Media']),
     ('Macro_pipe', Pipeline([
                           ('carryover', ExponentialCarryover()),
                           ('saturation', ExponentialSaturation())
     ]), ['Macro']),
     ('Micro_pipe', Pipeline([
                           ('carryover', ExponentialCarryover()),
                           ('saturation', ExponentialSaturation())
     ]), ['Micro']),
     ('Nano_pipe', Pipeline([
                           ('carryover', ExponentialCarryover()),
                           ('saturation', ExponentialSaturation())
     ]), ['Nano']),
    ],
    remainder='passthrough'
)

# Creating a new model with adstock effects
model5 = Pipeline([
                  ('adstock', adstock),
                  ('regression', Ridge(alpha=0.1))
])

# Setting up X and y variables
X2 = X1.copy()
y2 = y1.copy()

# Fitting X and y variables into the new model
model5.fit(X2, y2)

print(cross_val_score(model5, X2, y2, cv=TimeSeriesSplit()).mean())

# Installing the required module via pip
!pip install optuna==2.10.1

from optuna.integration import OptunaSearchCV
from optuna.distributions import UniformDistribution, IntUniformDistribution

# Implementing AdStock parameters on the previously fitted model using OptunaSearchCV
tuned_model = OptunaSearchCV(
    estimator = model5,
    param_distributions={
        'adstock__tv_pipe__carryover__strength': UniformDistribution(0, 1),
        'adstock__tv_pipe__carryover__length': IntUniformDistribution(0, 6),
        'adstock__tv_pipe__saturation__a': UniformDistribution(0, 0.01),
        'adstock__radio_pipe__carryover__strength': UniformDistribution(0, 1),
        'adstock__radio_pipe__carryover__length': IntUniformDistribution(0, 6),
        'adstock__radio_pipe__saturation__a': UniformDistribution(0, 0.01),
        'adstock__Social_Media_pipe__carryover__strength': UniformDistribution(0, 1),
        'adstock__Social_Media_pipe__carryover__length': IntUniformDistribution(0, 6),
        'adstock__Social_Media_pipe__saturation__a': UniformDistribution(0, 0.01),
        'adstock__Macro_pipe__carryover__strength': UniformDistribution(0, 1),
        'adstock__Macro_pipe__carryover__length': IntUniformDistribution(0, 6),
        'adstock__Macro_pipe__saturation__a': UniformDistribution(0, 0.01),
        'adstock__Micro_pipe__carryover__strength': UniformDistribution(0, 1),
        'adstock__Micro_pipe__carryover__length': IntUniformDistribution(0, 6),
        'adstock__Micro_pipe__saturation__a': UniformDistribution(0, 0.01),
        'adstock__Nano_pipe__carryover__strength': UniformDistribution(0, 1),
        'adstock__Nano_pipe__carryover__length': IntUniformDistribution(0, 6),
        'adstock__Nano_pipe__saturation__a': UniformDistribution(0, 0.01),
    },
    n_trials=100,
    cv=TimeSeriesSplit(),
    random_state=0
)

# Checking optimal cross validation accuracy through TimeSeriesSplit Cross validation
print(cross_val_score(tuned_model, X2, y2, cv=TimeSeriesSplit()).mean())

# Fitting X and y variables on the new Tuned Model
tuned_model.fit(X2, y2)

print(tuned_model.best_params_)
print(tuned_model.best_estimator_.named_steps['regression'].coef_)
print(tuned_model.best_estimator_.named_steps['regression'].intercept_)

- ## Printing best Parameter value for each X variable in a DataFrame

# Getting adstock parameters of the Tuned Model for each channel through a DataFrame
value = pd.DataFrame.from_dict(tuned_model.best_params_ , orient='index', columns=["value"] )
value

We can see from the above results, TV has the most carryover strength while Social Media & Radio have most carryover length. It means TV ads have high impact on Sales after initial spending, but they don't last long, quickly wear off in the same week. While Social Media and Radio have relatively less impact on Sales but they still have an effect on sales respectably 4 & 6 weeks later. Interestingly Macro influencers have both high carryover length and strength.

# applying get_value() function for tv saturation
tv_sat_a = value._get_value('adstock__tv_pipe__saturation__a', 'value')

# applying get_value() function for radio saturation
radio_sat_a = value._get_value('adstock__radio_pipe__saturation__a', 'value')

# applying get_value() function for social media saturation
Social_Media_sat_a = value._get_value('adstock__Social_Media_pipe__saturation__a', 'value')

# applying get_value() function for macro saturation
Macro_sat_a = value._get_value('adstock__Macro_pipe__saturation__a', 'value')

# applying get_value() function for micro saturation
Micro_sat_a = value._get_value('adstock__Micro_pipe__saturation__a', 'value')

# applying get_value() function for nano saturation
Nano_sat_a = value._get_value('adstock__Nano_pipe__saturation__a', 'value')
Nano_sat_a

- ## Calculating Exponents of all X variables

y_axis_TV = 1- np.exp(range(0,1100)*(-tv_sat_a))
y_axis_radio = 1- np.exp(range(0,1100)*(-radio_sat_a))
y_axis_Social_Media = 1- np.exp(range(0,1100)*(-Social_Media_sat_a))
y_axis_Macro = 1- np.exp(range(0,1100)*(-Macro_sat_a))
y_axis_Micro = 1- np.exp(range(0,1100)*(-Micro_sat_a))
y_axis_Nano = 1- np.exp(range(0,1100)*(-Nano_sat_a))

print(y_axis_TV)

# Saturation Effects

- ## Without Selecting the Dummy variables

import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
plt.figure(figsize=(15,6))

plt.plot(range(0,1100),y_axis_TV, label='TV')
plt.plot(range(0,1100),y_axis_radio, label='RADIO')
plt.plot(range(0,1100),y_axis_Social_Media, label='Social Media')

plt.legend()
plt.show()

We are getting saturation graph of tv radio and social media in same plot it is making sense to compare their saturation graphically.\
From here it seems like spending more than 300 is useless for Radio and spending more than 230 is useless for Social Media


- ## With using the Dummy variables

import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
plt.figure(figsize=(15,6))

plt.plot(range(0,1100),y_axis_TV, label='TV')
plt.plot(range(0,1100),y_axis_radio, label='RADIO')
plt.plot(range(0,1100),y_axis_Social_Media, label='Social Media')
plt.plot(range(0,1100),y_axis_Macro, label='Macro influencer')
plt.plot(range(0,1100),y_axis_Micro, label='Micro influencer')
plt.plot(range(0,1100),y_axis_Nano, label='Nano influencer')

plt.legend()
plt.show()

In the above chart, we are getting saturation graph of tv, radio, social media and other dummy influencers.

# Carryover Effects

> ### For TV

tv_carry_week = int(value._get_value('adstock__tv_pipe__carryover__length', 'value'))
w=100
tv_carry_strength = value._get_value('adstock__tv_pipe__carryover__strength', 'value')
val={0:100}
for i in range(1,tv_carry_week+1):
  val[i] = w*tv_carry_strength
  w=w*tv_carry_strength

week_no = list(val.keys())
values = list(val.values())
plt.bar(week_no, values)
val

> ### For Radio

radio_carry_week = int(value._get_value('adstock__radio_pipe__carryover__length', 'value'))
w=100
val={0:100}
radio_carry_strength = value._get_value('adstock__radio_pipe__carryover__strength', 'value')
for i in range(1,radio_carry_week+1):
  val[i] = w*radio_carry_strength
  w=w*radio_carry_strength

week_no = list(val.keys())
values = list(val.values())
plt.bar(week_no, values)
val

> ### For Social Media

Social_Media_carry_week = int(value._get_value('adstock__Social_Media_pipe__carryover__length', 'value'))
#Social_Media_carry_week = 6

w=100
Social_Media_carry_strength = value._get_value('adstock__Social_Media_pipe__carryover__strength', 'value')
val={0:100}

for i in range(1,Social_Media_carry_week+1):
  val[i] = w*Social_Media_carry_strength
  w=w*Social_Media_carry_strength

week_no = list(val.keys())
values = list(val.values())
plt.bar(week_no, values)
val

> ### For Macro Influencers

Macro_carry_week = int(value._get_value('adstock__Macro_pipe__carryover__length', 'value'))

w=100
Macro_carry_strength = value._get_value('adstock__Macro_pipe__carryover__strength', 'value')
val={0:100}

for i in range(1,Macro_carry_week+1):
  val[i] = w*Macro_carry_strength
  w=w*Macro_carry_strength

week_no = list(val.keys())
values = list(val.values())
plt.bar(week_no, values)
val

- ### For Micro Influencers

Micro_carry_week = int(value._get_value('adstock__Micro_pipe__carryover__length', 'value'))

w=100
Micro_carry_strength = value._get_value('adstock__Micro_pipe__carryover__strength', 'value')
val={0:100}

for i in range(1,Micro_carry_week+1):
  val[i] = w*Micro_carry_strength
  w=w*Micro_carry_strength

week_no = list(val.keys())
values = list(val.values())
plt.bar(week_no, values)
val



> ### For Nano Influencers


Nano_carry_week = int(value._get_value('adstock__Nano_pipe__carryover__length', 'value'))

w=100
Nano_carry_strength = value._get_value('adstock__Nano_pipe__carryover__strength', 'value')
val={0:100}

for i in range(1,Nano_carry_week+1):
  val[i] = w*Nano_carry_strength
  w=w*Nano_carry_strength

week_no = list(val.keys())
values = list(val.values())
plt.bar(week_no, values)
val

It seems that radio and Social Media advertisings still have an effect on sales respectably 4 & 6 weeks later after the initial spending. This is longer than the effect of TV spendings that quickly wear off in the same week.

# Channel Contributions

adstock_data = pd.DataFrame(
    tuned_model.best_estimator_.named_steps['adstock'].transform(X2),
    columns=X2.columns,
    index=X2.index
)
weights = pd.Series(
    tuned_model.best_estimator_.named_steps['regression'].coef_,
    index=X2.columns
)
base = tuned_model.best_estimator_.named_steps['regression'].intercept_
unadj_contributions = adstock_data.mul(weights).assign(Base=base)
adj_contributions = (unadj_contributions
                     .div(unadj_contributions.sum(axis=1), axis=0)
                     .mul(y, axis=0)
                    )
ax = (adj_contributions[['Base', 'Social_Media', 'Radio', 'TV', 'Macro', 'Micro', 'Nano']]
      .plot.area(
          figsize=(14, 8),
          linewidth=1,
          title='Predicted Sales and Breakdown',
          ylabel='Sales',
          xlabel='Date'
      )
     )

handles, labels = ax.get_legend_handles_labels()
ax.legend(
    handles[::-1], labels[::-1],
    title='Channels', loc="center left",
    bbox_to_anchor=(1.01, 0.5)
)

It seems Macro influencers have an higher impact on sales, even greater than TV which additionally still has an effect on sales 4 weeks later after initial spending.

# Comparing Actual and Predicted values

X_train3, X_test3, y_train3, y_test3 = train_test_split(X2, y2 ,test_size=0.35, random_state=101)
y_pred2 = tuned_model.predict(X_test3)
test3 = pd.DataFrame({'Predicted sales':y_pred2, 'Actual sales':y_test3})
fig= plt.figure(figsize=(12,6))
test3 = test3.reset_index()
test3 = test3.drop(['Date'],axis=1)
plt.plot(test3[:50])
plt.legend(['Actual sales','Predicted sales'])

# Checking For Multicollinearity using VIF values of the Explanatory Variables

from statsmodels.stats.outliers_influence import variance_inflation_factor
X3 = X2.copy()
X3['Constant'] = 1
vif = [variance_inflation_factor(X3.values, i) for i in range(len(X3.columns))]
pd.Series(vif, index=X3.columns)

From the above results we can clearly see there is no Multicollinearity issue between the independent variables (all values are less than 5)

# Using docplex to implement Solver in Python
Solver adjusts the values in the decision variable cells to satisfy the limits on constraint cells and produce the result you want for the objective cell.
Ex: - Determining the monthly product mix for a drug manufacturing unit that maximizes the profitability.

- ## Scenario:
Suppose we are a big multimillion dollar company, we want to spread awareness  of our products to customers by placing advertisements through Tv, Radio and Social Media. But we have a tight budget of, lets say $100000. We want to maximize our revenue, but we don't know how much budget we should allocate to each channel. Thats where Solver comes in.

- ### Necessary Libraries

!pip install docplex
!pip install cplex
from docplex.mp.model import Model

# Giving a model name
m = Model(name='Optimization_for_MMM')

Our Simple Linear Regression equation for the original data was:
> ### <b>Sales = 85.1463 + 3.4555 * TV + 0.98 * Radio + 2.469 * Social Media</b>

- ## Variables

# Adding the variables as integers
TV = m.integer_var(name='TV')
Radio = m.integer_var(name='Radio')
SM = m.integer_var(name='Social_Media')

- ## Constraints

> ### TV

# Adding constraints that Tv budget value should be non negative
TV_non_neg = m.add_constraint(TV >= 0)

> #### Radio

# Adding maximum and minimum budget constraint to radio
Radio_Min = m.add_constraint(Radio >= 120)
Radio_Max = m.add_constraint(Radio <= 300)
Radio_non_neg = m.add_constraint(Radio >= 0)

> ### Social Media Constraints

# Adding maximum and minimum budget constraint to Social Media
SM_Min = m.add_constraint(SM >= 100)
SM_Max = m.add_constraint(SM <= 230)
SM_non_neg = m.add_constraint(SM >= 0)

- ### Constraint on total ad Spent

# Fixing Total budget constraint
Total_budget_max = m.add_constraint(m.sum([TV + Radio + SM]) <= 100000)

- ### Coefficients from Linear Regression

TV_coef = 3.4555
Radio_coef = 0.98
SM_coef = 2.469
intercept = 85.1463

- ### Optimized Budget

# Maximizing the Sales/Revenue through the intercept and coefficients of linear regression
m.maximize(TV*TV_coef + Radio*Radio_coef + SM*SM_coef + intercept)

sol = m.solve()

# Printing the result
sol.display()

So, based on the results, if we have $100000 budget and we want to maximize our return of investment or objective, we should allocate 99780 dollars on Tv advertisements, 120 dollars on Radio advertisement and 100 dollars on Social Media advertisement.
